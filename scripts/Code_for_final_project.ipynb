{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://fisd-dataset.s3.amazonaws.com/fisd-asanti-twi-90p.zip\n",
        "!unzip fisd-asanti-twi-90p.zip"
      ],
      "metadata": {
        "id": "MKEUo9oELGrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "PBUF4DvJLKOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datasets import Dataset\n",
        "\n",
        "!pip install pandas datasets transformers"
      ],
      "metadata": {
        "id": "NzZOV0ofLOEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "#storing paths for csv and audio is variables\n",
        "train_csv_path = \"/content/fisd-asanti-twi-90p/data.csv\"\n",
        "audio_base_path = \"/content/fisd-asanti-twi-90p/audios/\"\n",
        "\n",
        "##########################################\n",
        "# Define cleaning function for training\n",
        "##########################################\n",
        "def clean_text_train(text, allowed_symbols=None):\n",
        "    \"\"\"\n",
        "    Cleans the input text by:\n",
        "    - Converting to uppercase.\n",
        "    - Normalizing Unicode characters.\n",
        "    - Removing non-alphanumeric characters except those specified in allowed_symbols.\n",
        "    - Stripping extra whitespace.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to clean.\n",
        "        allowed_symbols (set): A set of symbols to retain in the text.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    # Convert to uppercase\n",
        "    #text = text.upper()\n",
        "\n",
        "    # Normalize Unicode to NFC\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "    # Define default allowed characters if none provided\n",
        "    if allowed_symbols is None:\n",
        "        allowed_symbols = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ƆƐabcdefghijklmnopqrstuvwxyzɛɔ \")\n",
        "\n",
        "    # Create a set of allowed characters (letters, digits, space, and allowed symbols)\n",
        "    allowed_chars = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ƆƐabcdefghijklmnopqrstuvwxyzɛɔ\").union(allowed_symbols)\n",
        "\n",
        "    # Remove characters not in allowed_chars\n",
        "    cleaned = \"\".join(ch for ch in text if ch in allowed_chars)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "# Load training CSV file\n",
        "\n",
        "train_df = pd.read_csv(train_csv_path, delimiter=\"\\t\", on_bad_lines='skip')\n",
        "\n",
        "# Remove 'Unnamed: 0' column if it exists\n",
        "train_df = train_df.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "\n",
        "# Print a few rows before cleaning for verification\n",
        "print(\"=== Sample Rows Before Cleaning ===\")\n",
        "print(train_df.head(5))\n",
        "\n",
        "\n",
        "# Update 'Audio Filepath' by removing the lacounda\n",
        "train_df['Audio Filepath'] = train_df['Audio Filepath'].apply(\n",
        "    lambda x: os.path.join(audio_base_path, os.path.basename(x))\n",
        ")\n"
      ],
      "metadata": {
        "id": "p9UWWg8cLWVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the 'Transcription' column\n",
        "# Specify allowed symbols (add any symbols you want to retain)\n",
        "# Example: periods, commas, percent, ampersand, hyphen, Ghanaian Cedi symbol\n",
        "allowed_symbols_train = set(\".,\")\n",
        "\n",
        "train_df['Transcription'] = train_df['Transcription'].apply(\n",
        "    lambda x: clean_text_train(x, allowed_symbols=allowed_symbols_train) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "# Printing a few rows after cleaning to verify the cleaning step\n",
        "print(\"\\n=== Sample Rows After Cleaning ===\")\n",
        "print(train_df.head(5))\n",
        "\n",
        "\n",
        "# Shuffling and splitting the dataset\n",
        "\n",
        "shuffled_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "split_ratio = 0.8  # 80% for training\n",
        "split_index = int(len(shuffled_df) * split_ratio)\n",
        "\n",
        "train_df = shuffled_df[:split_index]       # Training data\n",
        "validation_df = shuffled_df[split_index:]  # Validation data\n",
        "\n",
        "\n",
        "# Converting to match Hugging Face Datasets\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "validation_dataset = Dataset.from_pandas(validation_df)\n",
        "\n",
        "common_voice = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": validation_dataset\n",
        "})\n",
        "\n",
        "print(common_voice)\n",
        "\n",
        "\n",
        "# Printing a few samples from the train dataset for final verification\n",
        "\n",
        "print(\"\\n=== Sample Training Examples ===\")\n",
        "for i in range(min(3, len(train_dataset))):\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(\"Audio Filepath:\", train_dataset[i][\"Audio Filepath\"])\n",
        "    print(\"Transcription:\", train_dataset[i][\"Transcription\"])\n",
        "    print()\n",
        "\n",
        "\n",
        "# Print a few samples from the validation dataset for final verification\n",
        "\n",
        "if len(validation_dataset) > 0:\n",
        "    print(\"\\n=== Sample Validation Examples ===\")\n",
        "    for i in range(min(3, len(validation_dataset))):\n",
        "        print(f\"Example {i+1}\")\n",
        "        print(\"Audio Filepath:\", validation_dataset[i][\"Audio Filepath\"])\n",
        "        print(\"Transcription:\", validation_dataset[i][\"Transcription\"])\n",
        "        print()\n",
        "else:\n",
        "    print(\"\\nNo validation examples found.\")\n",
        "\n",
        "#looking at the features after clean up\n",
        "common_voice[\"train\"].features\n",
        "\n",
        "from datasets import Audio\n",
        "\n",
        "# If you don't need the Translation column, remove it\n",
        "common_voice = common_voice.remove_columns([\"Translation\"])\n",
        "\n",
        "# Rename columns to match the source format\n",
        "common_voice = common_voice.rename_column(\"Audio Filepath\", \"audio\")\n",
        "common_voice = common_voice.rename_column(\"Transcription\", \"sentence\")\n",
        "\n",
        "# Casting the audio column to an Audio feature.\n",
        "# Specifying the sampling rate of 16kHz\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "# Printing the features to confirm\n",
        "print(common_voice[\"train\"].features)\n",
        "\n",
        "import os\n",
        "from datasets import Audio\n",
        "#clearing bad files that will cause errors:\n",
        "\n",
        "# Names of the bad files\n",
        "bad_files = [\n",
        "    \"AsantiTwiFm20-A SLRKMb-Tmp012-ql0LnX.ogg\",\n",
        "    \"AsantiTwiFm20-A SLRKMb-Tmp089-eRlzWZ.ogg\",\n",
        "    \"AsantiTwiFm20-A SLRKMb-Tmp011-bcTYyx.ogg\",\n",
        "    \"AsantiTwiFm18-MHdyb4ij-Tmp001-6afLtE.ogg\",\n",
        "    \"AsantiTwiMa20-SQF23O2h-Tmp014-T6agOK.ogg\",\n",
        "\n",
        "]\n",
        "\n",
        "# Disabling audio decoding so that filtering won't try to load the audio\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(decode=False))\n",
        "\n",
        "# Filtering out the rows with the bad files\n",
        "common_voice = common_voice.filter(\n",
        "    lambda x: os.path.basename(x[\"audio\"][\"path\"]) not in bad_files\n",
        ")\n",
        "\n",
        "# Re-enabling audio decoding after  removing bad files\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000, decode=True))\n",
        "\n",
        "# Printing to verify the dataset no longer contains those files\n",
        "print(common_voice)\n",
        "\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "# Do not specify 'language' or set it to None\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", task=\"transcribe\")\n",
        "\n",
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "    example = processor(\n",
        "        audio=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        text=example[\"sentence\"],\n",
        "    )\n",
        "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
        "    return example\n",
        "\n",
        "common_voice = common_voice.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=common_voice[\"train\"].column_names,\n",
        "    num_proc=1\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "22z5A7aqLhzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LIbhsyaWHHl2",
        "outputId": "617a7894-a20c-4e07-dbf5-923741bb9801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-05 07:03:03--  https://fisd-dataset.s3.amazonaws.com/fisd-asanti-twi-90p.zip\n",
            "Resolving fisd-dataset.s3.amazonaws.com (fisd-dataset.s3.amazonaws.com)... 16.15.184.212, 3.5.3.161, 54.231.129.17, ...\n",
            "Connecting to fisd-dataset.s3.amazonaws.com (fisd-dataset.s3.amazonaws.com)|16.15.184.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 233403440 (223M) [application/zip]\n",
            "Saving to: ‘fisd-asanti-twi-90p.zip’\n",
            "\n",
            "fisd-asanti-twi-90p  11%[=>                  ]  26.58M  40.9MB/s               ^C\n",
            "Archive:  fisd-asanti-twi-90p.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of fisd-asanti-twi-90p.zip or\n",
            "        fisd-asanti-twi-90p.zip.zip, and cannot find fisd-asanti-twi-90p.zip.ZIP, period.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%%capture` not found.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any  # This should be an instance of WhisperProcessor\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        This method takes a batch (list) of examples as input, each example being a dictionary\n",
        "        containing at least 'input_features' and 'labels'. It returns a dictionary of padded\n",
        "        tensors ready for model input.\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract the input features (audio features). Each example has 'input_features' as a list with one element.\n",
        "        input_features = [{\"input_features\": f[\"input_features\"][0]} for f in features]\n",
        "\n",
        "        # Pad the input features to a common length using the processor's feature extractor.\n",
        "        # This creates a batch with 'input_features' as a padded tensor (mel spectrogram for Whisper).\n",
        "        batch = self.processor.feature_extractor.pad(\n",
        "            input_features,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Extract labels. Each example has a 'labels' field, which is a list of token IDs.\n",
        "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
        "\n",
        "        # Pad the labels using the processor’s tokenizer. This returns a dictionary with\n",
        "        # 'input_ids' and 'attention_mask' for the labels.\n",
        "        labels_batch = self.processor.tokenizer.pad(\n",
        "            label_features,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Replace padding token IDs with -100 so that they are ignored by the loss function.\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        # If all sequences start with a BOS (beginning-of-sequence) token, remove it.\n",
        "        # This is sometimes done to align with how the model expects input.\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        # Add the processed labels to the batch.\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n",
        "!pip install evaluate\n",
        "\n",
        "import evaluate\n",
        "\n",
        "# Load the Word Error Rate (WER) metric\n",
        "metric = evaluate.load(\"wer\")\n"
      ],
      "metadata": {
        "id": "J9odS4JsLqeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have a custom normalizer for Twi, define it here.\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import torch\n",
        "import evaluate\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import WhisperProcessor\n",
        "from datasets import Audio, Dataset, DatasetDict\n",
        "\n",
        "\n",
        "# Initialising Processor and Metric\n",
        "# Assuming we have chosen a multilingual model capable of Twi and English code switching to train it on our twi dataset.\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    \"openai/whisper-small\",\n",
        "    task=\"transcribe\"  # no language specified, letting Whisper detect language\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"wer\")\n",
        "\n",
        "#Defining the Twi Financial Normalizer\n",
        "\n",
        "def twi_financial_normalizer(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "    # Allowed characters include letters, digits, financial symbols, punctuation used in financial contexts\n",
        "    allowed_chars = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ƆƐ\")\n",
        "    text = \"\".join(ch for ch in text if ch in allowed_chars)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "8B5vWVGSK7Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    # Extracting predictions and labels\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Replacing -100 with the pad token id so that we decode properly\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # Decoding predictions and references\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Normalizing predictions and references\n",
        "    pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]\n",
        "    label_str_norm = [twi_financial_normalizer(l) for l in label_str]\n",
        "\n",
        "    # Filtering out empty references to avoid errors in WER computation\n",
        "    pred_str_norm = [\n",
        "        pred_str_norm[i] for i in range(len(pred_str_norm))\n",
        "        if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "    label_str_norm = [\n",
        "        label_str_norm[i] for i in range(len(label_str_norm))\n",
        "        if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    # Computing orthographic WER (no normalization)\n",
        "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    cer = 100 * compute_character_error_rate(pred_str, label_str)\n",
        "\n",
        "    # Computing WER on normalized text\n",
        "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
        "\n",
        "    return {\"wer_ortho\": wer_ortho, \"wer\": wer, \"cer\": cer}\n"
      ],
      "metadata": {
        "id": "DtGHNmxjKzbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   # Computing CER (Character Error Rate)\n",
        "    # We define a helper function to compute CER for a list of predictions and references\n",
        "\n",
        "def compute_character_error_rate(predictions, references):\n",
        "    \"\"\"\n",
        "    Compute the Character Error Rate (CER) between a list of predictions and references.\n",
        "    CER is defined as the edit distance between the predicted and reference text,\n",
        "    divided by the number of characters in the reference, multiplied by 100.\n",
        "    \"\"\"\n",
        "    total_distance = 0\n",
        "    total_chars = 0\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        # Remove extra spaces if needed and measure distance\n",
        "        pred = pred.strip()\n",
        "        ref = ref.strip()\n",
        "\n",
        "        # Compute edit distance for characters\n",
        "        dist = levenshtein_distance(ref, pred)\n",
        "        total_distance += dist\n",
        "        total_chars += len(ref)\n",
        "\n",
        "    # Avoid division by zero if some references are empty\n",
        "    if total_chars == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return total_distance / total_chars\n",
        "\n",
        "\n",
        "def levenshtein_distance(s1, s2):\n",
        "    \"\"\"\n",
        "    Compute the Levenshtein distance (edit distance) between two strings at the character level.\n",
        "    This is a standard dynamic programming approach:\n",
        "    - distance[i][j] represents the edit distance between s1[:i] and s2[:j].\n",
        "    \"\"\"\n",
        "    m = len(s1)\n",
        "    n = len(s2)\n",
        "\n",
        "    # Initialize a (m+1)x(n+1) matrix\n",
        "    distance = np.zeros((m+1, n+1), dtype=int)\n",
        "\n",
        "    # Base cases: distance from empty string\n",
        "    for i in range(m+1):\n",
        "        distance[i][0] = i\n",
        "    for j in range(n+1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # Compute distances\n",
        "    for i in range(1, m+1):\n",
        "        for j in range(1, n+1):\n",
        "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
        "            distance[i][j] = min(\n",
        "                distance[i-1][j] + 1,     # deletion\n",
        "                distance[i][j-1] + 1,     # insertion\n",
        "                distance[i-1][j-1] + cost # substitution\n",
        "            )\n",
        "\n",
        "    return distance[m][n]\n",
        "    return {\"wer\": wer, \"cer\": cer}\n",
        "\n",
        "#This is the final code for compute_metrics:\n",
        "def compute_metrics(pred):\n",
        "    # Extracting predictions and labels\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Replacing -100 with the pad token id so that we decode properly\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # Decode predictions and references\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Normalising predictions and references\n",
        "    pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]\n",
        "    label_str_norm = [twi_financial_normalizer(l) for l in label_str]\n",
        "\n",
        "    # Filtering out empty references to avoid errors in WER computation\n",
        "    pred_str_norm = [\n",
        "        pred_str_norm[i] for i in range(len(pred_str_norm))\n",
        "        if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "    label_str_norm = [\n",
        "        label_str_norm[i] for i in range(len(label_str_norm))\n",
        "        if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "    # Ensuring to initialize metrics dictionary\n",
        "    metrics = {}\n",
        "\n",
        "    # Computing orthographic WER (no normalization)\n",
        "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    metrics[\"wer_ortho\"] = wer_ortho # Add metric to dictionary\n",
        "\n",
        "    # Computing CER\n",
        "    cer = 100 * compute_character_error_rate(pred_str, label_str)\n",
        "    metrics[\"cer\"] = cer # Add metric to dictionary\n",
        "\n",
        "    # Computing WER on normalized text\n",
        "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
        "    metrics[\"wer\"] = wer # Add metric to dictionary\n",
        "\n",
        "    return metrics # Return the metrics dictionary"
      ],
      "metadata": {
        "id": "nYqbZNgHKsiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n",
        "!pip install evaluate\n",
        "\n",
        "import evaluate\n",
        "\n",
        "# Loading the Word Error Rate (WER) metric\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "mNvf4mcaL55-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defing our custon twi normaliser around here:\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import torch\n",
        "import evaluate\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import WhisperProcessor\n",
        "from datasets import Audio, Dataset, DatasetDict\n",
        "\n",
        "\n",
        "# Initialising Processor and Metric\n",
        "\n",
        "# Assume you have chosen a multilingual model capable of Twi and English code switching.\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    \"openai/whisper-small\",\n",
        "    task=\"transcribe\"  # no language specified, letting Whisper detect language\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"wer\")\n",
        "\n",
        "\n",
        "# Defining the Twi Financial Normalizer\n",
        "\n",
        "def twi_financial_normalizer(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "    # Allowed characters include letters, digits, financial symbols, punctuation used in financial contexts\n",
        "    allowed_chars = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ƆƐ\")\n",
        "    text = \"\".join(ch for ch in text if ch in allowed_chars)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "0Gu-1vGdKiMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the Compute Metrics Function\n",
        "\n",
        "# def compute_metrics(pred):\n",
        "#     pred_ids = pred.predictions\n",
        "#     label_ids = pred.label_ids\n",
        "\n",
        "#     # Replace -100 with pad_token_id\n",
        "#     label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "#     # Decode predictions and references\n",
        "#     pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "#     label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "#     # Compute orthographic WER (no normalization)\n",
        "#     wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "#     # Normalize predictions and references\n",
        "#     pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]\n",
        "#     label_str_norm = [twi_financial_normalizer(l) for l in label_str]\n",
        "\n",
        "#     # Filter out empty references to avoid errors in WER computation\n",
        "#     pred_str_norm = [\n",
        "#         pred_str_norm[i] for i in range(len(pred_str_norm))\n",
        "#         if len(label_str_norm[i]) > 0\n",
        "#     ]\n",
        "#     label_str_norm = [\n",
        "#         label_str_norm[i] for i in range(len(label_str_norm))\n",
        "#         if len(label_str_norm[i]) > 0\n",
        "#     ]\n",
        "\n",
        "#     # Compute WER on normalized text\n",
        "#     wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
        "\n",
        "#     return {\"wer_ortho\": wer_ortho, \"wer\": wer}\n",
        "\n",
        "\n",
        "# def compute_metrics(pred):\n",
        "#     # Extract predictions and labels\n",
        "#     pred_ids = pred.predictions\n",
        "#     label_ids = pred.label_ids\n",
        "\n",
        "#     # Replace -100 with the pad token id so that we decode properly\n",
        "#     label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "#     # Decode predictions and references\n",
        "#     pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "#     label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "#     # Normalize predictions and references\n",
        "#     pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]\n",
        "#     label_str_norm = [twi_financial_normalizer(l) for l in label_str]\n",
        "\n",
        "#     # Filter out empty references to avoid errors in WER computation\n",
        "#     pred_str_norm = [\n",
        "#         pred_str_norm[i] for i in range(len(pred_str_norm))\n",
        "#         if len(label_str_norm[i]) > 0\n",
        "#     ]\n",
        "#     label_str_norm = [\n",
        "#         label_str_norm[i] for i in range(len(label_str_norm))\n",
        "#         if len(label_str_norm[i]) > 0\n",
        "#     ]\n",
        "\n",
        "# def compute_character_error_rate(predictions, references):\n",
        "#     \"\"\"\n",
        "#     Compute the Character Error Rate (CER) between a list of predictions and references.\n",
        "#     CER is defined as the edit distance between the predicted and reference text,\n",
        "#     divided by the number of characters in the reference, multiplied by 100.\n",
        "#     \"\"\"\n",
        "#     total_distance = 0\n",
        "#     total_chars = 0\n",
        "#     for pred, ref in zip(predictions, references):\n",
        "#         # Remove extra spaces if needed and measure distance\n",
        "#         pred = pred.strip()\n",
        "#         ref = ref.strip()\n",
        "\n",
        "        # Compute edit distance for characters\n",
        "#         dist = levenshtein_distance(ref, pred)\n",
        "#         total_distance += dist\n",
        "#         total_chars += len(ref)\n",
        "\n",
        "#     # Avoid division by zero if some references are empty\n",
        "#     if total_chars == 0:\n",
        "#         return 0.0\n",
        "\n",
        "#     return total_distance / total_chars\n",
        "\n",
        "\n",
        "# def levenshtein_distance(s1, s2):\n",
        "#     \"\"\"\n",
        "#     Compute the Levenshtein distance (edit distance) between two strings at the character level.\n",
        "#     This is a standard dynamic programming approach:\n",
        "#     - distance[i][j] represents the edit distance between s1[:i] and s2[:j].\n",
        "#     \"\"\"\n",
        "#     m = len(s1)\n",
        "#     n = len(s2)\n",
        "\n",
        "#     # Initialize a (m+1)x(n+1) matrix\n",
        "#     distance = np.zeros((m+1, n+1), dtype=int)\n",
        "\n",
        "#     # Base cases: distance from empty string\n",
        "#     for i in range(m+1):\n",
        "#         distance[i][0] = i\n",
        "#     for j in range(n+1):\n",
        "#         distance[0][j] = j\n",
        "\n",
        "#     # Compute distances\n",
        "#     for i in range(1, m+1):\n",
        "#         for j in range(1, n+1):\n",
        "#             cost = 0 if s1[i-1] == s2[j-1] else 1\n",
        "#             distance[i][j] = min(\n",
        "#                 distance[i-1][j] + 1,     # deletion\n",
        "#                 distance[i][j-1] + 1,     # insertion\n",
        "#                 distance[i-1][j-1] + cost # substitution\n",
        "#             )\n",
        "\n",
        "#     return distance[m][n]\n",
        "\n",
        "#     # Compute orthographic WER (no normalization)\n",
        "#     wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "#     cer = 100 * compute_character_error_rate(pred_str, label_str)\n",
        "\n",
        "#     # Compute WER on normalized text\n",
        "#     wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
        "\n",
        "#     return {\"wer_ortho\": wer_ortho, \"wer\": wer, \"cer\": cer}\n",
        "#     # Compute CER (Character Error Rate)\n",
        "#     # We define a helper function to compute CER for a list of predictions and references\n"
      ],
      "metadata": {
        "id": "ZZWHBrj4Kdvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "# disabling cache during training since it's incompatible with gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "\n",
        "# setting task for generation and re-enable cache\n",
        "# removing 'language=\"sinhalese\"' since we're allowing Whisper to detect Twi automatically\n",
        "model.generate = partial(\n",
        "    model.generate,\n",
        "    task=\"transcribe\",\n",
        "    use_cache=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "oaM5U3XZKXex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/please\",    # directory to save models\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,      # adjust as needed for your GPU resources\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_steps=500, #change back to 400\n",
        "    #max_steps=500,                      # increase if you have more training resources/time\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    fp16_full_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=500, #after every 500 steps the model at that time\n",
        "    eval_steps=500, #evaluate the model after 500 steps\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit =15,\n",
        "    num_train_epochs=2\n",
        "    # push_to_hub=True,                   # Set to False if you do not want to push to Hugging Face Hub\n",
        ")\n",
        "\n",
        "# disabling cache during training since it's incompatible with gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "\n",
        "# setting task for generation and re-enable cache\n",
        "# We do not specify language here to allow automatic language detection\n",
        "model.generate = partial(\n",
        "    model.generate,\n",
        "    task=\"transcribe\",\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "from transformers import Seq2SeqTrainer\n",
        "#using the Seq2Seq Trainer since audio is an input sequence and we are outputting a sequence of words\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor\n",
        ")\n",
        "#trainer.processing_class = processor.__class__\n",
        "\n",
        "#training\n",
        "trainer.train()\n",
        "\n",
        "# If resuming from checkpoint if it terminates unexpectedly\n",
        "trainer.train(resume_from_checkpoint=True)"
      ],
      "metadata": {
        "id": "UQZ3TAEGHUVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}